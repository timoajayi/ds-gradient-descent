{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you can see how the three variants of Gradient Descent (Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent) can be implemented in Python. \n",
    "Remember the iterative steps of Gradient Descent to better understand the code.\n",
    "It might also help you to understand the influence of the hyper-parameters of Gradient Descent by trying to modify some of them here.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Read through the notebook and try to understand what happens at each step. As you can see proper comments describing the code are missing. Add them wherever its necessary (or helpful for you to understand the code). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:00.373928Z",
     "start_time": "2020-04-22T12:35:00.370054Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `plt.style.use` you can set layouts (background color, gird, etc.) for the whole notebook. In this notebook we will use the ggplot layout. But there are more ([click here](https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:01.286386Z",
     "start_time": "2020-04-22T12:35:01.277915Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use gradient descent to find optimal parameters for a linear regression problem. Here we will create our own data so that we know what the values of the parameters should be for a perfect fit.\n",
    "\n",
    "$y_{i}=\\beta_{0} + \\beta_{1}*x_{i} + \\varepsilon_{i}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate some data with:\n",
    "\n",
    "$\\beta_{0} = 4$ and  $\\beta_{1} = 3$\n",
    "\n",
    "We'll also add some Gaussian noise to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:02.964345Z",
     "start_time": "2020-04-22T12:35:02.960199Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# create 100 data points for X\n",
    "X = 2 * np.random.rand(100,1)\n",
    "# y = b_0 + b_1*X + random noise\n",
    "y = 4 + 3 * X + np.random.randn(100,1)\n",
    "print(\"X dimensions: \", X.shape)\n",
    "print(\"y dimensions: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our data to check the relation between X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the 100 data points we have just created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analytical way of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that there is a closed formula to find the best parameters for linear regression, called the **Normal-Equation**:\n",
    "\n",
    "\n",
    "$$ b = (X^TX)^{-1}X^Ty $$\n",
    "\n",
    "*Remember:* [How to multiply matrices](https://www.mathsisfun.com/algebra/matrix-multiplying.html) \n",
    "\n",
    "We were using this formula before to solve linear regression tasks. \n",
    "Let's try it out and see which results for $b_{0}$ and $b_{1}$ it will come up with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:05.132827Z",
     "start_time": "2020-04-22T12:35:05.125992Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.c_ can be used to concatenate arrays\n",
    "X_b = np.c_[np.ones((100,1)), X]\n",
    "print(\"X_b dimensions: \", X_b.shape)\n",
    "\n",
    "# Here we implement the formula above with numpy .T for transpose and .dot for the dot product\n",
    "# and .linalg.inv for matrix inversion \n",
    "b_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "print(b_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is close to our real parameters 4 and 3. It cannot be accurate due to the noise we have added to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:06.308429Z",
     "start_time": "2020-04-22T12:35:06.301858Z"
    }
   },
   "outputs": [],
   "source": [
    "X_new = np.array([[0],[2]])\n",
    "X_new_b = np.c_[np.ones((2,1)), X_new]\n",
    "y_predict = X_new_b.dot(b_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a prediction line with the calculated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:07.303442Z",
     "start_time": "2020-04-22T12:35:07.154356Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_new, y_predict, 'r-')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Now, let's use gradient descent to find the optimal parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function & Gradients\n",
    "\n",
    "The general equations for calculating the cost function and gradients for linear regression are shown below. \n",
    "\n",
    "**Please note the cost function is for Linear regression. For other algorithms the cost function will be different and the gradients would have to be derived from the cost functions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Cost**\n",
    "\n",
    "$$J(b) = \\frac{ 1 }{ 2n } \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "**Gradient**\n",
    "\n",
    "$$ \\frac{\\partial J(b)}{\\partial b_j} = \\frac{ 1 }{ n }\\sum_{i=1}^{n}(\\hat{y}_i - y_i) x_{i,j} $$\n",
    "\n",
    "\n",
    "**Updating the parameters with the gradients**\n",
    "\n",
    "$$ b_0: = b_0 -\\alpha (\\frac{ 1 }{ n } \\sum_{i=1}^{n}(\\hat{y}_i - y_i) * 1) $$\n",
    "\n",
    "$$ b_1: = b_1 -\\alpha (\\frac{ 1 }{ n } \\sum_{i=1}^{n}(\\hat{y}_i - y_i)  x_{i,1})$$\n",
    "\n",
    "\n",
    "$$ b_2: = b_2 -\\alpha (\\frac{ 1 }{ n } \\sum_{i=1}^{n}(\\hat{y}_i - y_i) x_{i,2})$$\n",
    "\n",
    "\n",
    "\n",
    "$$ b_j: = b_j -\\alpha (\\frac{ 1 }{ n } \\sum_{i=1}^{n}(\\hat{y}_i - y_i) x_{i,j})$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the three variants of the gradient descent method are applied: **batch gradient descent**, **stochastic gradient decent** and **mini-batch gradient descent**. Remind yourself which steps are iteratively repeated in the gradient descent procedure.\n",
    "\n",
    "## Batch Gradient Descent\n",
    "\n",
    "Batch gradient descent, computes the gradient of the cost function with respect to the parameters $ b $ for the entire training dataset:\n",
    "\n",
    "$$ b = b - \\alpha \\frac{\\partial}{\\partial b} J(b) $$\n",
    "\n",
    "As we need to calculate the gradients for the whole dataset to perform just one update, batch gradient descent can be very slow and is intractable for datasets that don't fit in memory. Batch gradient descent also doesn't allow us to update our model online, i.e. with new examples on-the-fly.\n",
    "\n",
    "We then update our parameters in the opposite direction of the gradients with the learning rate determining how big of an update we perform. Batch gradient descent is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:09.625198Z",
     "start_time": "2020-04-22T12:35:09.619623Z"
    }
   },
   "outputs": [],
   "source": [
    "def cal_cost(b, X, y):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error cost for linear regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    b : ndarray of shape (j, 1)\n",
    "        Parameter vector (including bias weight).\n",
    "    X : ndarray of shape (n, j)\n",
    "        Feature matrix including the bias column (all ones).\n",
    "    y : ndarray of shape (n, 1)\n",
    "        True target values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost : float\n",
    "        The mean squared error cost.\n",
    "    \"\"\"\n",
    "\n",
    "    n = y.shape[0]\n",
    "    \n",
    "    predictions = X.dot(b)\n",
    "    cost = 1/(2*n) * np.sum(np.square(predictions-y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:11.186126Z",
     "start_time": "2020-04-22T12:35:11.179051Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, b, learning_rate=0.01, iterations=1000):\n",
    "    \"\"\"\n",
    "        Perform batch gradient descent for linear regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n, j)\n",
    "            Feature matrix including bias column.\n",
    "        y : ndarray of shape (n, 1)\n",
    "            Target values.\n",
    "        b : ndarray of shape (j, 1)\n",
    "            Initial parameter vector.\n",
    "        learning_rate : float, optional\n",
    "            Step size for parameter updates.\n",
    "        iterations : int, optional\n",
    "            Number of gradient descent iterations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        b : ndarray of shape (j, 1)\n",
    "            Final learned parameter vector.\n",
    "        cost_history : ndarray of shape (iterations,)\n",
    "            Cost value at each iteration.\n",
    "        b_history : ndarray of shape (iterations, j)\n",
    "            Parameter values at each iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = y.shape[0]\n",
    "    cost_history = np.zeros(iterations)\n",
    "    b_history = np.zeros((iterations, b.shape[0]))\n",
    "    \n",
    "    for it in range(iterations):\n",
    "        \n",
    "        prediction = np.dot(X,b)\n",
    "        \n",
    "        b = b - learning_rate * (1/n) * (X.T.dot((prediction - y)))\n",
    "        b_history[it,:] = b.T\n",
    "        cost_history[it]  = cal_cost(b, X, y)\n",
    "        \n",
    "    return b, cost_history, b_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with 1000 iterations and a learning rate of 0.01. Start with initial parameters from a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:13.065084Z",
     "start_time": "2020-04-22T12:35:13.040039Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_iter = 1000\n",
    "\n",
    "b = np.random.randn(2,1)\n",
    "\n",
    "X_b = np.c_[np.ones((len(X),1)),X]\n",
    "b, cost_history, b_history = batch_gradient_descent(X_b, y, b, lr, n_iter)\n",
    "\n",
    "\n",
    "print(f\"b_0:             {b[0, 0]:0.3f},\\n\"\n",
    "      f\"b_1:             {b[1, 0]:0.3f}\")\n",
    "\n",
    "print(f\"Final cost:      {cost_history[-1]:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the cost history over iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:14.850238Z",
     "start_time": "2020-04-22T12:35:14.697374Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "ax.set_ylabel('J$(b)$')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.plot(range(n_iter), cost_history, 'b.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After around 150 iterations the cost is flat so the remaining iterations are not needed or will not result in any further optimization. Let us zoom in till iteration 200 and see the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:16.502195Z",
     "start_time": "2020-04-22T12:35:16.343148Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "ax.set_ylabel('J$(b)$')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.plot(range(200), cost_history[:200], 'b.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth pointing out that the cost drops faster initially and then the gain in cost reduction is not so great. \n",
    "\n",
    "It would be great to see the effect of different learning rates and iterations together. Let us  build a function which can show the effects together and also show how gradient decent is actually working.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:19.187496Z",
     "start_time": "2020-04-22T12:35:19.177691Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_GD(n_iter,lr, ax, ax1=None):\n",
    "    \"\"\"\n",
    "    Plot gradient descent steps for linear regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iter : int\n",
    "        Number of gradient descent iterations.\n",
    "    lr : float\n",
    "        Learning rate.\n",
    "    ax : matplotlib.axes.Axes\n",
    "        Axis for plotting data points and prediction lines.\n",
    "    ax1 : matplotlib.axes.Axes, optional\n",
    "        Axis for plotting cost vs iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.plot(X, y, 'b.')\n",
    "    b = np.random.randn(2,1)\n",
    "\n",
    "    transparency = 0.1\n",
    "    cost_history = np.zeros(n_iter)\n",
    "    for i in range(n_iter):\n",
    "        #pred_prev = X_b.dot(b)\n",
    "        b, h, _ = batch_gradient_descent(X_b, y, b, lr, 1)\n",
    "        pred = X_b.dot(b)\n",
    "\n",
    "        cost_history[i] = h[0]\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            ax.plot(X, pred, 'r-', alpha=transparency)\n",
    "            if transparency < 0.8:\n",
    "                transparency = transparency+0.2\n",
    "    if ax1 is not None:\n",
    "        ax1.plot(range(n_iter), cost_history, 'b.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the graphs for different iterations and learning rates combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:24.844832Z",
     "start_time": "2020-04-22T12:35:20.821679Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,25), dpi=200)\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "iterations_lr = [(2000,0.001), (500,0.01), (200,0.05), (100,0.1)]\n",
    "\n",
    "for i, (n_iter, lr) in enumerate(iterations_lr):\n",
    "    ax  = fig.add_subplot(4, 2, 2*i + 1)\n",
    "    ax1 = fig.add_subplot(4, 2, 2*i + 2)\n",
    "\n",
    "    ax.set_title(f\"lr: {lr}\")\n",
    "    ax1.set_title(f\"Iterations: {n_iter}\")\n",
    "\n",
    "    plot_GD(n_iter, lr, ax, ax1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how useful it is to visualize the effect of learning rates and iterations on gradient descent. The red lines show how the gradient descent starts and then slowly gets closer to the final value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T09:38:15.809824Z",
     "start_time": "2018-08-17T09:38:15.807296Z"
    }
   },
   "source": [
    "You can always plot individual graphs to zoom in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:35:27.732286Z",
     "start_time": "2020-04-22T12:35:27.535367Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(14,10))\n",
    "plot_GD(100, 0.1, ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent (SGD) in contrast performs a parameter update for each training example $ x_i $ and label $ y_i $:\n",
    "\n",
    "$$ b = b - \\alpha \\frac{\\partial}{\\partial b} J(b; x_i; y_i ) $$\n",
    "\n",
    "Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online.\n",
    "\n",
    "While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD's fluctuation, on the one hand, enables it to jump to new and potentially better local minima. On the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behavior as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively.\n",
    "\n",
    "Feel free to change some parameters and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:37:02.726329Z",
     "start_time": "2020-04-22T12:37:02.718041Z"
    }
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, b, learning_rate=0.01, epochs=10):\n",
    "    \"\"\"\n",
    "    Perform stochastic gradient descent for linear regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (n, j)\n",
    "        Feature matrix with bias column.\n",
    "    y : ndarray of shape (n, 1)\n",
    "        Target values.\n",
    "    b : ndarray of shape (j, 1)\n",
    "        Initial parameter vector.\n",
    "    learning_rate : float\n",
    "        Learning rate.\n",
    "    epochs : int\n",
    "        Number of passes over the training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    b : ndarray of shape (j, 1)\n",
    "        Final parameter vector.\n",
    "    cost_history : ndarray of shape (epochs,)\n",
    "        Cost value at the end of each epoch.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    cost_history = np.zeros(epochs)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        cost = 0.0\n",
    "        for _ in range(n):\n",
    "            rand_ind = np.random.randint(0, n)\n",
    "            #print(\"rand_ind:\", rand_ind)\n",
    "            X_i = X[rand_ind,:].reshape(1, X.shape[1])\n",
    "            #print(\"X_i shape:\", X_i.shape)\n",
    "            y_i = y[rand_ind].reshape(1, 1)\n",
    "            #print(\"y_i shape:\", y_i.shape)\n",
    "            prediction = np.dot(X_i, b)\n",
    "\n",
    "            b = b - (1/1) * learning_rate * (X_i.T.dot((prediction - y_i)))\n",
    "            cost = cost +  cal_cost(b, X_i, y_i)\n",
    "        cost_history[epoch]  = cost/n\n",
    "        \n",
    "    return b, cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:37:05.883324Z",
     "start_time": "2020-04-22T12:37:03.090142Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 2000\n",
    "\n",
    "b = np.random.randn(2,1)\n",
    "X_b = np.c_[np.ones((len(X),1)), X]\n",
    "b, cost_history = stochastic_gradient_descent(X_b, y, b, lr, epochs)\n",
    "\n",
    "print(f'b_0: {b[0][0]:0.3f},\\nb_1: {b[1][0]:0.3f}')\n",
    "print(f'Final cost  {cost_history[-1]:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:37:06.040819Z",
     "start_time": "2020-04-22T12:37:05.895582Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.set_ylabel('J$(b)$', rotation=0)\n",
    "ax.set_xlabel('Epochs')\n",
    "b = np.random.randn(2,1)\n",
    "\n",
    "ax.plot(range(epochs), cost_history, 'b.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batch Gradient Descent\n",
    "\n",
    "Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of \n",
    "$ m $ training examples.\n",
    "\n",
    "$$ b = b - \\alpha \\frac{\\partial}{\\partial b} J(b; x_{(i:i+m)}; y_{(i:i+m)} ) $$\n",
    "\n",
    "This way,  \n",
    "1. reduces the variance of the parameter updates, which can lead to more stable convergence\n",
    "2. can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient with respect to a mini-batch very efficient. \n",
    "\n",
    "Common mini-batch sizes range between 50 and 256, but can vary for different applications. Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:36:09.440324Z",
     "start_time": "2020-04-22T12:36:09.430176Z"
    }
   },
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X, y, b, learning_rate=0.01, epochs=10, batch_size=20):\n",
    "    \"\"\"\n",
    "    Perform mini-batch gradient descent for linear regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (n, j)\n",
    "        Feature matrix without bias column.\n",
    "    y : ndarray of shape (n, 1)\n",
    "        Target values.\n",
    "    b : ndarray of shape (j, 1)\n",
    "        Initial parameter vector.\n",
    "    learning_rate : float\n",
    "        Learning rate.\n",
    "    epochs : int\n",
    "        Number of passes over the training data.\n",
    "    batch_size : int\n",
    "        Number of samples per mini-batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    b : ndarray of shape (j, 1)\n",
    "        Final learned parameter vector.\n",
    "    cost_history : ndarray of shape (epochs,)\n",
    "        True cost on the FULL dataset at the end of each epoch.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    cost_history = np.zeros(epochs)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        cost = 0.0\n",
    "        shuffled_indices = np.random.permutation(n)\n",
    "        X = X[shuffled_indices]\n",
    "        y = y[shuffled_indices]\n",
    "        for ind, i in enumerate(range(0, n, batch_size)):\n",
    "            print(f\"iteration nr:{ind},in epoch nr:{epoch}\")\n",
    "            X_i = X[i:i+batch_size]\n",
    "            y_i = y[i:i+batch_size]\n",
    "            \n",
    "            X_i = np.c_[np.ones(len(X_i)),X_i]\n",
    "           \n",
    "            prediction = np.dot(X_i, b)\n",
    "\n",
    "            b = b - (1/batch_size) * learning_rate * (X_i.T.dot((prediction - y_i)))\n",
    "            cost += cal_cost(b, X_i, y_i)\n",
    "        cost_history[epoch]  = cost/(ind+1)     # cost/number of batches\n",
    "        \n",
    "    return b, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:36:16.712777Z",
     "start_time": "2020-04-22T12:36:09.996397Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 2000\n",
    "\n",
    "b = np.random.randn(2,1)\n",
    "\n",
    "\n",
    "b, cost_history = minibatch_gradient_descent(X, y, b, lr, epochs, batch_size=20)\n",
    "\n",
    "\n",
    "print(f'b_0: {b[0][0]:0.3f},\\nb_1: {b[1][0]:0.3f}')\n",
    "print(f'Final cost:  {cost_history[-1]:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:36:31.946149Z",
     "start_time": "2020-04-22T12:36:31.801306Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.set_ylabel('J$(b)$', rotation=0)\n",
    "ax.set_xlabel('Epochs')\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "ax.plot(range(epochs), cost_history, 'b.');"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Gradient Descent-Python",
    "public": true
   },
   "id": ""
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "461.183px",
    "left": "846.167px",
    "right": "138.333px",
    "top": "127px",
    "width": "559.667px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
