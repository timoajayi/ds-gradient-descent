{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of a basic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using gradient descent, we can run into some problems: \n",
    "\n",
    "- Getting trapped in a local minimum.\n",
    "\n",
    "- Overshooting and missing the global optimum. This is a direct result of moving too fast along the gradient direction.\n",
    "\n",
    "- Oscillation: This is a phenomenon that occurs when the function's value doesn't change significantly no matter the direction it advances.\n",
    "\n",
    "To combat these problems, a momentum term $ \\alpha $ is used to stabilize the rate of learning when moving towards the global optimum value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a `gradient_descent()` function. In this function, the loop ends when either:\n",
    "\n",
    "- The number of iterations exceeds a maximum value.\n",
    "\n",
    "- The difference in function values between two successive iterations falls below a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(max_iterations, threshold, b_init,\n",
    "                     obj_func, grad_func,\n",
    "                     learning_rate=0.05, momentum=0.8):\n",
    "    ''' \n",
    "    Input:\n",
    "    - max_iterations  : Maximum number of iterations to run\n",
    "    - threshold       : Stop if the difference in function values between two successive iterations falls below this threshold\n",
    "    - b_init          : Initial point from where to start gradient descent\n",
    "    - obj_func        : Reference to the function that computes the objective function\n",
    "    - grad_func       : Reference to the function that computes the gradient of the function\n",
    "    - learning_rate   : Step size for gradient descent. It should be in [0,1]\n",
    "    - momentum        : Momentum to use. It should be in [0,1]\n",
    "    Output:\n",
    "    - b_history       : All points in space, visited by gradient descent at which the objective function was evaluated\n",
    "    - f_history       : Corresponding value of the objective function computed at each point\n",
    "    '''\n",
    "    \n",
    "    b = b_init\n",
    "    b_history = b\n",
    "    f_history = obj_func(b)\n",
    "    delta_b = np.zeros(b.shape)\n",
    "    i = 0\n",
    "    diff = 1.0e10\n",
    "   \n",
    "    while  i<max_iterations and diff>threshold:\n",
    "        delta_b = -learning_rate*grad_func(b) + momentum*delta_b\n",
    "        b = b + delta_b\n",
    "        \n",
    "        # store the history of w and f\n",
    "        b_history = np.vstack((b_history, b))\n",
    "        f_history = np.vstack((f_history, obj_func(b)))\n",
    "        \n",
    "        # update iteration number and diff between successive values\n",
    "        # of objective function\n",
    "        i += 1\n",
    "        diff = np.absolute(f_history[-1] - f_history[-2])\n",
    "    \n",
    "    return b_history, f_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run this general purpose implementation of gradient descent on one example 2D function $ f(b_1 , b_2) = b_1^2 + b_2^2$ with circular contours.\n",
    "\n",
    "The function has a minimum value of zero at the origin. Let's visualize the function first and then find its minimum value with gradient descent.\n",
    "\n",
    "The ```visualize_fw()``` function below, generates 2500 equally spaced points on a grid and computes the function value at each point.\n",
    "\n",
    "The ```function_plot()``` function displays all points in different colors, depending upon the value of $f(b_1 , b_2)$\n",
    " at that point. All points at which the function's value is the same, have the same color:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fw():\n",
    "    xcoord = np.linspace(-10.0, 10.0, 50)\n",
    "    ycoord = np.linspace(-10.0, 10.0, 50)\n",
    "    b_1, b_2 = np.meshgrid(xcoord, ycoord)\n",
    "    pts = np.vstack((b_1.flatten(), b_2.flatten()))\n",
    "    \n",
    "    # All 2D points on the grid\n",
    "    pts = pts.transpose()\n",
    "    \n",
    "    # Function value at each point\n",
    "    f_vals = np.sum(pts*pts, axis=1)\n",
    "    function_plot(pts, f_vals)\n",
    "    plt.title('Objective Function Shown in Color')\n",
    "    plt.show()\n",
    "    return pts, f_vals\n",
    "\n",
    "# Helper function to annotate a single point\n",
    "def annotate_pt(text, xy, xytext, color):\n",
    "    plt.plot(xy[0], xy[1], marker='P', markersize=10, c=color)\n",
    "    plt.annotate(text, xy=xy, xytext=xytext,\n",
    "                 arrowprops=dict(arrowstyle=\"->\",\n",
    "                 color=color,\n",
    "                 connectionstyle='arc3'))\n",
    "\n",
    "# Plot the function\n",
    "# Pts are 2D points and f_val is the corresponding function value\n",
    "def function_plot(pts, f_val):\n",
    "    f_plot = plt.scatter(pts[:,0], pts[:,1],\n",
    "                         c=f_val, vmin=min(f_val), vmax=max(f_val),\n",
    "                         cmap='RdBu_r')\n",
    "    plt.colorbar(f_plot)\n",
    "    # Show the optimal point\n",
    "    annotate_pt('global minimum', (0,0), (-5,-7), 'yellow')    \n",
    "\n",
    "pts, f_vals = visualize_fw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the gradient descent function to minimize our objective function. To call `gradient_descent()`, we define two functions:\n",
    "\n",
    "`f()`: Computes the objective function at any point `b`\n",
    "\n",
    "`grad()`: Computes the gradient at any point `b`\n",
    "\n",
    "To understand the effect of various hyper-parameters on gradient descent, the function `solve_fw()` calls `gradient_descent()` with 5 iterations for different values of learning rate and momentum.\n",
    "\n",
    "\n",
    "## What is Momentum?\n",
    "\n",
    "SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.\n",
    "\n",
    "<img src=\"./images/no-momentum.gif\" alt=\"https://mlfromscratch.com/optimizers-explained/#/\" width=\"500\"/>\n",
    "<img src=\"./images/momentum.gif\" alt=\"https://mlfromscratch.com/optimizers-explained/#/\" width=\"500\"/>\n",
    "\n",
    "Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in the images above. It does this by adding a fraction \n",
    "$ \\gamma $ of the update vector of the past time step to the current update vector:\n",
    "\n",
    "$$ v_{t} = \\gamma v_{t-1} + \\alpha \\frac{\\partial}{\\partial b} J(b)  $$\n",
    "$$  b_{t} = b_{t-1} - v_{t} $$\n",
    "\n",
    "*Note: Some implementations exchange the signs in the equations. The momentum term $ \\gamma $ is usually set to a value between 0 and 1.*\n",
    "\n",
    "Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. $ \\gamma < 1 $. The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.\n",
    "\n",
    "\n",
    "If you want to play with momentum and learning rate, I recommend visiting distill's page for [Why Momentum Really Work](https://distill.pub/2017/momentum/).\n",
    "\n",
    "\n",
    "\n",
    "The function `visualize_learning()`, plots the values of \n",
    "$ (b_1 , b_2) $, with function values shown in different colors. The arrows in the plot make it easier to track which point was updated from the last:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function\n",
    "def f(b):\n",
    "    return np.sum(b*b)\n",
    "\n",
    "# Function to compute the gradient\n",
    "def grad(b):\n",
    "    return 2*b\n",
    "\n",
    "# Function to plot the objective function\n",
    "# and learning history annotated by arrows\n",
    "# to show how learning proceeded\n",
    "def visualize_learning(b_history):  \n",
    "    \n",
    "    # Make the function plot\n",
    "    function_plot(pts, f_vals)\n",
    "    \n",
    "    # Plot the history\n",
    "    plt.plot(b_history[:,0], b_history[:,1], marker='o', c='magenta') \n",
    "    \n",
    "    # Annotate the point found at last iteration\n",
    "    annotate_pt('minimum found',\n",
    "                (b_history[-1,0], b_history[-1,1]),\n",
    "                (-1,7), 'green')\n",
    "    iter = b_history.shape[0]\n",
    "    for b, i in zip(b_history, range(iter-1)):\n",
    "        # Annotate with arrows to show history\n",
    "        plt.annotate(\"\",\n",
    "                    xy=b, xycoords='data',\n",
    "                    xytext=b_history[i+1,:], textcoords='data',\n",
    "                    arrowprops=dict(arrowstyle='<-',\n",
    "                            connectionstyle='angle3'))     \n",
    "    \n",
    "def solve_fw():\n",
    "    # Setting up\n",
    "    rand = np.random.RandomState(19)\n",
    "    b_init = rand.uniform(-10,10,2)\n",
    "    fig, ax = plt.subplots(nrows=4, ncols=5, figsize=(18, 12))\n",
    "    learning_rates = [0.05, 0.2, 0.5, 0.8, 1]\n",
    "    momentum = [0, 0.2, 0.5, 0.9]\n",
    "    ind = 1\n",
    "    \n",
    "    # Iteration through all possible parameter combinations\n",
    "    for alpha in momentum:\n",
    "        for eta, col in zip(learning_rates, [0, 1, 2, 3, 4]):\n",
    "            plt.subplot(4, 5, ind)        \n",
    "            b_history, f_history = gradient_descent(5, -1, b_init, f, grad, eta, alpha)\n",
    "            \n",
    "            visualize_learning(b_history)\n",
    "            ind = ind+1\n",
    "            plt.text(-9, 12, 'Learning Rate = ' + str(eta), fontsize=13)\n",
    "            if col==1:\n",
    "                plt.text(10, 15, 'momentum = ' + str(alpha), fontsize=20)\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=.3)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run ```solve_fw()``` and see how the learning rate and momentum effect gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_fw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example clarifies the role of both momentum and learning rate.\n",
    "\n",
    "In the first plot, with zero momentum and learning rate set at 0.05, learning is slow and the algorithm does not reach the global minimum. Increasing the momentum speeds up learning as we can see from the plots in the first column. The other extreme is the last column, where the learning rate is kept high. This causes oscillations, which can be controlled to a certain extent by adding momentum.\n",
    "\n",
    "The general guideline for gradient descent is to use small values of learning rate and higher values of momentum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "677d31ef4c22b30b595e6ac105a0015f6ca73648dea406c285666f4499ccd873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
