{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write your own Gradient Descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm\n",
    "\n",
    "A simple gradient Descent Algorithm looks like this:\n",
    "\n",
    "\n",
    "1. Obtain a function to minimize F(x)\n",
    "\n",
    "2. Initialize a value x from which to start the descent or optimization from\n",
    "\n",
    "3. Specify a learning rate that will determine how much of a step to descend by or how quickly you converge to the minimum value\n",
    "\n",
    "4. Obtain the derivative of that value x (the descent)\n",
    "\n",
    "5. Proceed to descend by the derivative of that value multiplied by the learning rate\n",
    "\n",
    "6. Adjust the value of x\n",
    "\n",
    "7. Check your stop condition to see whether to stop\n",
    "\n",
    "8. If condition satisfied, stop. If not, proceed to step 4 with the new x value and keep repeating algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement this in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a simple representation of gradient descent using python. \n",
    "\n",
    "We will create an arbitrary loss function and attempt to find a local minimum value the range from -1 and 3 for that function \n",
    "f(x) = x³ — 3x² + 7 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "We will first visualize this function with a set of values ranging from -1 and 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# creating the function and plotting it \n",
    "\n",
    "def function(x):\n",
    "    y = ...\n",
    "    return y\n",
    "\n",
    "\n",
    "# Get 1000 evenly spaced numbers between -1 and 3 \n",
    "x = ...\n",
    "\n",
    "# Plot the curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "We will then proceed to make two functions for the gradient descent implementation.\n",
    "\n",
    "The first is a derivative function: \n",
    "\n",
    "This function takes in a value of x and returns its derivative based on the initial function we specified. It is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv(x):\n",
    "    \"\"\"\n",
    "    Compute the derivative of the function\n",
    "\n",
    "        f(x) = x³ - 3x²\n",
    "\n",
    "    The derivative is\n",
    "\n",
    "        f´(x) = 3x² - 6x\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Input value at which the derivative is evaluated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_deriv: The value of the derivative f′(x) evaluated at x.\n",
    "    \"\"\"\n",
    "    # Compute the derivative\n",
    "    x_deriv = ...\n",
    "    return x_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is the function where the actual gradient descent takes place. \n",
    "\n",
    "This function takes in an initial or previous value for x, updates it based on the step taken via the descent multiplied by the learning rate and outputs the most minimum value of x that reaches the stop condition. \n",
    "\n",
    "\n",
    "\n",
    "For our stop condition, we are going to use a precision stop.\n",
    "\n",
    "\n",
    "\n",
    "This means that when the absolute difference between our old and updated x is smaller than a value, the algorithm should stop. \n",
    "\n",
    "\n",
    "\n",
    "The function will also print out the minimum value of x as well as the number of steps or descents it took to reach that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "def gradient_descent(x_new, x_prev, precision, l_r):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    This function performs gradient descent starting from an initial value of x.\n",
    "    At each step, x is updated using the derivative of the function multiplied by\n",
    "    the learning rate. The algorithm continues until a precision-based stopping\n",
    "    condition is satisfied.\n",
    "\n",
    "    The stopping condition is met when the absolute difference between successive\n",
    "    x values is smaller than the specified precision.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    x_new : Initial (current) value of x that will be updated during gradient descent.\n",
    "    x_prev : Previous value of x used to check the stopping condition.\n",
    "    precision : Threshold that determines when gradient descent should stop.\n",
    "    l_r : Learning rate (size of each descent step).\n",
    "        \n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    1. Prints the final x value, which corresponds to the local minimum reached.\n",
    "    2. Prints the total number of gradient descent steps taken.\n",
    "    3. Plots the gradient descent path over the function curve.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store all x values during gradient descent\n",
    "    x_list = [...]\n",
    "\n",
    "    # Store corresponding y = f(x) values\n",
    "    y_list = [...]\n",
    "\n",
    "    # Continue updating x until the change between iterations is smaller than precision\n",
    "    while ... > precision:\n",
    "        \n",
    "        # Store current x value\n",
    "        x_current = ...\n",
    "\n",
    "        # Compute derivative of the function at current x\n",
    "        x_deriv = ...\n",
    "\n",
    "        # Update x using gradient descent\n",
    "        # new x = current x - (learning rate * derivative)\n",
    "        x_new = ...\n",
    "        \n",
    "        # Update previous x for precision check\n",
    "        x_prev = ...\n",
    "\n",
    "        # Append the updated x value to the list for plotting the descent path\n",
    "        ...\n",
    "\n",
    "        # Compute and store the corresponding y value for the updated x\n",
    "        ...\n",
    "\n",
    "    print (\"Local minimum occurs at: \"+ str(x_new))\n",
    "    print (\"Number of steps: \" + str(len(x_list)))\n",
    "    \n",
    "    # Create plot to show the gradient descent path \n",
    "    # over the function curve \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "\n",
    "Now we will use our two functions and see if they are working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement gradient descent (all the arguments are arbitrarily chosen)\n",
    "\n",
    "gradient_descent(x_new=0.5, x_prev=0, precision=0.001, l_r=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
